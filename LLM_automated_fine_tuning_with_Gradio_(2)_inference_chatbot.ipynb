{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2qMoGwCOIwE"
   },
   "source": [
    "This notebook uses **unsloth**, a framework that helps fine-tuning LLMs faster with less memory.\n",
    "\n",
    "<a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/\n",
    "unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Wri7bZxVOGbs"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dFWH4yAfOMe9"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from huggingface_hub import get_token, whoami\n",
    "from unsloth import FastLanguageModel\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3sI8xz6RWjdU"
   },
   "outputs": [],
   "source": [
    "# Load the fine-tuned model and its tokenizer\n",
    "finetuned_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=whoami()[\"name\"]\n",
    "    + \"/\"\n",
    "    + \"fine-tuned-model\",  # change the model's name if necessary\n",
    "    token=get_token(),\n",
    "    max_seq_length=256,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    ")\n",
    "FastLanguageModel.for_inference(finetuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Eqk09voirX3"
   },
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def respond(message, history):\n",
    "        current_history = history + [(message, None)]\n",
    "        messages = \"\"\n",
    "        for user, assistant in current_history:\n",
    "            messages += \"<|user|>\\n\" + user + \"</s>\\n\" + \"<|assistant|>\\n\"\n",
    "            if assistant is not None:\n",
    "                messages += assistant + \"</s>\\n\"\n",
    "\n",
    "        inputs = tokenizer(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = finetuned_model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
    "        bot_message = tokenizer.batch_decode(outputs)[0].split(\"<|assistant|>\\n\")[-1]\n",
    "\n",
    "        history.append((message, bot_message))\n",
    "        time.sleep(2)\n",
    "        return None, history\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "demo.launch(share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

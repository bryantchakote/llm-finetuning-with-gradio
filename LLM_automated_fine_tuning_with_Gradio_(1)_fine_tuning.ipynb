{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-702dC9sSPsx"
      },
      "source": [
        "This notebook uses **unsloth**, a framework that helps fine-tuning LLMs faster with less memory.\n",
        "\n",
        "<a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/\n",
        "unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup**"
      ],
      "metadata": {
        "id": "JQQnC0MbXTLR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zo8HaE_WQsfY"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YD1OY0yekoy"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import gc\n",
        "import random\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from transformers import (\n",
        "    LlamaForCausalLM,\n",
        "    MistralForCausalLM,\n",
        "    GemmaForCausalLM,\n",
        "    PreTrainedTokenizerFast,\n",
        "    LlamaTokenizerFast,\n",
        "    GemmaTokenizerFast,\n",
        ")\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "from huggingface_hub import get_token\n",
        "\n",
        "from unsloth import FastLanguageModel, PatchDPOTrainer\n",
        "from peft import PeftModelForCausalLM, LoftQConfig\n",
        "\n",
        "from datasets import load_dataset, Dataset\n",
        "from trl import DPOTrainer\n",
        "\n",
        "import gradio as gr\n",
        "from gradio import Textbox, Dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ml1SsCjK--I"
      },
      "source": [
        "# **Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA6e6X6Bdbet",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Some params\n",
        "\n",
        "# Dataset\n",
        "dataset_name_list = [\n",
        "    \"argilla/distilabel-intel-orca-dpo-pairs\",\n",
        "    \"Intel/orca_dpo_pairs\",\n",
        "    \"jondurbin/py-dpo-v0.1\",\n",
        "    \"jondurbin/truthy-dpo-v0.1\",\n",
        "]\n",
        "\n",
        "# Model\n",
        "model_name_list = [\n",
        "    \"unsloth/tinyllama-bnb-4bit\",  # very small model for fast testing\n",
        "    # Llama\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "    \"unsloth/yi-6b-bnb-4bit\",\n",
        "    # Mistral\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.2-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
        "    \"unsloth/zephyr-sft-bnb-4bit\",\n",
        "    \"unsloth/OpenHermes-2.5-Mistral-7B-bnb-4bit\",\n",
        "    \"unsloth/Hermes-2-Pro-Mistral-7B-bnb-4bit\",\n",
        "    # Gemma\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-it-bnb-4bit\",\n",
        "    \"unsloth/codegemma-7b-bnb-4bit\",\n",
        "]  # More models at https://huggingface.co/unsloth\n",
        "\n",
        "\n",
        "# Exponents of 2 between 2^start and 2^end both included\n",
        "def exp2_list(start: int = 0, end: int = 0) -> List[int]:\n",
        "    \"\"\"\n",
        "    Generate a list of powers of 2 from 2^start to 2^end both included.\n",
        "\n",
        "    Args:\n",
        "      start (int, optional): The starting exponent. Defaults to 0.\n",
        "      end (int, optional): The ending exponent. Defaults to 0.\n",
        "\n",
        "    Returns:\n",
        "      List[int]: A list containing the powers of 2 from 2^start to 2^end.\n",
        "    \"\"\"\n",
        "    assert isinstance(start, int), \"start must be an integer\"\n",
        "    assert isinstance(end, int), \"end must be an integer\"\n",
        "    assert start >= 0, \"start must be non-negative\"\n",
        "    assert end >= start, \"end must be greater than or equal to start\"\n",
        "\n",
        "    return [2**n for n in range(start, end + 1)]\n",
        "\n",
        "\n",
        "# Target modules to be trained during the fine-tuning\n",
        "target_module_list = [\n",
        "    \"q_proj\",\n",
        "    \"k_proj\",\n",
        "    \"v_proj\",\n",
        "    \"o_proj\",\n",
        "    \"gate_proj\",\n",
        "    \"up_proj\",\n",
        "    \"down_proj\",\n",
        "]\n",
        "\n",
        "# Gradient checkpointing values\n",
        "use_gradient_checkpointing_list = [\"unsloth\", True]\n",
        "\n",
        "# Optimizers\n",
        "optim_list = [\"adamw_8bit\", \"paged_adamw_8bit\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evFlifkxhzwr",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Get dataset\n",
        "\n",
        "\n",
        "def get_dataset(\n",
        "    dataset_name: str,\n",
        "    max_samples: int = 1000,\n",
        "    test_size: float = 0.2,\n",
        "    train_test_split_seed: int = 42,\n",
        ") -> Tuple[Textbox, Textbox, Textbox, Dataset, Dataset]:\n",
        "    \"\"\"\n",
        "    Load a dataset, limit it to a maximum number of samples, transform it to match DPO input requirements, and perform train-test split.\n",
        "\n",
        "    Args:\n",
        "        dataset_name (str): The name of the dataset to load.\n",
        "        max_samples (int, optional): The maximum number of samples to keep from the dataset. Defaults to 1000.\n",
        "        test_size (float, optional): The proportion of the dataset to include in the test split. Defaults to 0.2.\n",
        "        train_test_split_seed (int, optional): The random seed for the train-test split. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[gr.Textbox, gr.Textbox, gr.Textbox, datasets.DatasetDict, datasets.DatasetDict]: A tuple containing three Textbox objects representing a sample's prompt, chosen, and rejected responses, followed by two Datasets objects representing the training and evaluation datasets.\n",
        "    \"\"\"\n",
        "    gr.Info(\"Getting dataset.\")\n",
        "    try:\n",
        "        # Load dataset\n",
        "        dataset = load_dataset(dataset_name, split=\"train\")\n",
        "        gr.Info(\"Raw data successfully loaded.\")\n",
        "\n",
        "        # Limit to max_samples rows\n",
        "        sample_size = min(max_samples, len(dataset))\n",
        "        dataset = dataset.select(range(sample_size))\n",
        "        gr.Info(f\"{sample_size} rows kept.\")\n",
        "\n",
        "        # Transform the data to match the DPO models' requirements\n",
        "        # (columns: prompt, chosen, rejected)\n",
        "        column_names = dataset.column_names\n",
        "        dataset = dataset.map(\n",
        "            return_prompt_and_responses,\n",
        "            num_proc=16,\n",
        "            remove_columns=column_names,\n",
        "        )\n",
        "        gr.Info(\"Data transformed successfully.\")\n",
        "\n",
        "        # Train test split\n",
        "        dataset = dataset.train_test_split(\n",
        "            test_size=test_size, seed=train_test_split_seed\n",
        "        )\n",
        "        train_dataset = dataset[\"train\"]\n",
        "        eval_dataset = dataset[\"test\"]\n",
        "        gr.Info(\"Train test split done.\")\n",
        "\n",
        "        # View a data sample\n",
        "        sample = random.choice(train_dataset)\n",
        "        sample_prompt = sample[\"prompt\"]\n",
        "        sample_chosen = sample[\"chosen\"]\n",
        "        sample_rejected = sample[\"rejected\"]\n",
        "\n",
        "        sample_prompt = gr.Textbox(value=sample_prompt, visible=True)\n",
        "        sample_chosen = gr.Textbox(value=sample_chosen, visible=True)\n",
        "        sample_rejected = gr.Textbox(value=sample_rejected, visible=True)\n",
        "\n",
        "        gr.Info(\"Data is ready.\")\n",
        "\n",
        "        return (\n",
        "            sample_prompt,\n",
        "            sample_chosen,\n",
        "            sample_rejected,\n",
        "            train_dataset,\n",
        "            eval_dataset,\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        raise gr.Error(f\"Error getting dataset: {str(e)}\")\n",
        "\n",
        "\n",
        "# Prepare dataset for DPO fine-tuning\n",
        "def return_prompt_and_responses(samples: Dict[str, str]) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Extract prompt and responses from a dictionary of samples.\n",
        "\n",
        "    Args:\n",
        "      samples (Dict[str, str]): A dictionary containing sample data.\n",
        "\n",
        "    Returns:\n",
        "      Dict[str, str]: A dictionary containing the prompt and the chosen and rejected responses.\n",
        "    \"\"\"\n",
        "    prompt = samples.get(\"prompt\")\n",
        "    if prompt is None:\n",
        "        prompt = samples.get(\"question\")\n",
        "    if prompt is None:\n",
        "        prompt = samples.get(\"input\")\n",
        "\n",
        "    # Apply chat template\n",
        "    prompt = \"<|user|>\\n\" + prompt + \"</s>\\n<|assistant|>\\n\"\n",
        "    chosen = samples[\"chosen\"] + \"</s>\\n\"\n",
        "    rejected = samples[\"rejected\"] + \"</s>\\n\"\n",
        "\n",
        "    return {\n",
        "        \"prompt\": prompt,\n",
        "        \"chosen\": chosen,\n",
        "        \"rejected\": rejected,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rawdzFoWtuB",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Get model\n",
        "\n",
        "\n",
        "def get_model(\n",
        "    model_name: str,\n",
        "    token: str,\n",
        "    max_seq_length: int = 256,\n",
        "    load_in_4bit: bool = True,\n",
        ") -> Tuple[\n",
        "    LlamaForCausalLM | MistralForCausalLM | GemmaForCausalLM,\n",
        "    PreTrainedTokenizerFast | LlamaTokenizerFast | GemmaTokenizerFast,\n",
        "]:\n",
        "    \"\"\"\n",
        "    Load a language model and its tokenizer.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): The name or path of the pre-trained model to load.\n",
        "        token (str): The authentication token for loading the model.\n",
        "        max_seq_length (int, optional): The maximum sequence length for tokenization. Defaults to 256.\n",
        "        load_in_4bit (bool, optional): Whether to load the model in 4-bit precision. Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[LlamaForCausalLM | MistralForCausalLM | GemmaForCausalLM, PreTrainedTokenizerFast | LlamaTokenizerFast | GemmaTokenizerFast]: A tuple containing the loaded language model and its tokenizer.\n",
        "    \"\"\"\n",
        "    gr.Info(\"Getting model.\")\n",
        "    try:\n",
        "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name=model_name,\n",
        "            token=token,\n",
        "            max_seq_length=max_seq_length,\n",
        "            load_in_4bit=load_in_4bit,\n",
        "            dtype=None,\n",
        "        )\n",
        "\n",
        "        gr.Info(\"Model is ready.\")\n",
        "\n",
        "        return model, tokenizer\n",
        "\n",
        "    except Exception as e:\n",
        "        raise gr.Error(f\"Error getting model: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_uIe6FHicsD",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Get PEFT model\n",
        "\n",
        "\n",
        "def get_peft_model(\n",
        "    model: LlamaForCausalLM | MistralForCausalLM | GemmaForCausalLM,\n",
        "    r: int = 8,\n",
        "    lora_alpha: int = 8,\n",
        "    peft_model_random_state: int = 42,\n",
        "    target_modules: List[str] = [\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    loftq_config: None | LoftQConfig = None,\n",
        "    use_rslora: bool = False,\n",
        "    lora_dropout: float = 0,\n",
        "    bias: str = \"none\",\n",
        "    use_gradient_checkpointing: str | bool = \"unsloth\",\n",
        ") -> PeftModelForCausalLM:\n",
        "    \"\"\"\n",
        "    Load a PEFT model for causal language modeling.\n",
        "\n",
        "    Args:\n",
        "        model (LlamaForCausalLM | MistralForCausalLM | GemmaForCausalLM,): The base model for which PEFT modifications are applied.\n",
        "        r (int, optional): The number of heads to use in PEFT. Defaults to 8.\n",
        "        lora_alpha (int, optional): The alpha parameter for LoRA. Defaults to 8.\n",
        "        peft_model_random_state (int, optional): Random seed for PEFT model initialization. Defaults to 42.\n",
        "        target_modules (List[str], optional): The list of target modules for PEFT modifications. Defaults to [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"].\n",
        "        loftq_config (Optional[LoftQConfig], optional): The configuration for LoRA-Fine-Tuning-Aware Quantization. Defaults to None.\n",
        "        use_rslora (bool, optional): Whether to use rank stabilized LoRA. Defaults to False.\n",
        "        lora_dropout (float, optional): The dropout rate for LoRA. Defaults to 0.\n",
        "        bias (str, optional): The type of bias to use. Defaults to \"none\".\n",
        "        use_gradient_checkpointing (str | bool, optional): Whether to use gradient checkpointing. Defaults to \"unsloth\".\n",
        "\n",
        "    Returns:\n",
        "        PeftModelForCausalLM: The PEFT model for causal language modeling.\n",
        "    \"\"\"\n",
        "    gr.Info(\"Getting PEFT model.\")\n",
        "    try:\n",
        "        peft_model = FastLanguageModel.get_peft_model(\n",
        "            model,\n",
        "            r=r,\n",
        "            lora_alpha=lora_alpha,\n",
        "            random_state=peft_model_random_state,\n",
        "            target_modules=target_modules,\n",
        "            loftq_config=loftq_config,\n",
        "            use_rslora=use_rslora,\n",
        "            lora_dropout=lora_dropout,\n",
        "            bias=bias,\n",
        "            use_gradient_checkpointing=use_gradient_checkpointing,\n",
        "        )\n",
        "\n",
        "        gr.Info(\"PEFT model is ready.\")\n",
        "\n",
        "        return peft_model\n",
        "\n",
        "    except Exception as e:\n",
        "        raise gr.Error(f\"Error getting PEFT model: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NwqW0qt53CO",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Get DPO trainer\n",
        "\n",
        "\n",
        "def get_dpo_trainer(\n",
        "    peft_model: PeftModelForCausalLM,\n",
        "    tokenizer: PreTrainedTokenizerFast | LlamaTokenizerFast | GemmaTokenizerFast,\n",
        "    train_dataset: Dataset,\n",
        "    eval_dataset: Dataset,\n",
        "    per_device_train_batch_size: int = 4,\n",
        "    num_train_epochs: int = 3,\n",
        "    learning_rate: float = 5e-6,\n",
        "    logging_steps: int = 50,\n",
        "    optim: str = \"adamw_8bit\",\n",
        "    dpo_trainer_seed: int = 42,\n",
        "    beta: float = 0.1,\n",
        "    max_length: int = 256,\n",
        "    max_prompt_length: int = 128,\n",
        "    output_dir: str = \"fine-tuned-model\",\n",
        "    push_to_hub: bool = True,\n",
        "    gradient_accumulation_steps: int = 4,\n",
        "    warmup_ratio: float = 0.1,\n",
        "    weight_decay: float = 0.0,\n",
        "    lr_scheduler_type: str = \"linear\",\n",
        ") -> DPOTrainer:\n",
        "    \"\"\"\n",
        "    Load a DPO trainer.\n",
        "\n",
        "    Args:\n",
        "        peft_model (PeftModelForCausalLM): The PEFT model for training.\n",
        "        tokenizer (PreTrainedTokenizerFast | LlamaTokenizerFast | GemmaTokenizerFast): The tokenizer corresponding to the model.\n",
        "        train_dataset (Dataset): The training dataset.\n",
        "        eval_dataset (Dataset): The evaluation dataset.\n",
        "        per_device_train_batch_size (int, optional): Batch size per GPU/CPU for training. Defaults to 4.\n",
        "        num_train_epochs (int, optional): Number of training epochs. Defaults to 5.\n",
        "        learning_rate (float, optional): Learning rate. Defaults to 5e-6.\n",
        "        logging_steps (int, optional): Log every n steps. Defaults to 50.\n",
        "        optim (str, optional): Optimizer type. Defaults to \"adamw_8bit\".\n",
        "        dpo_trainer_seed (int, optional): Seed for DPO trainer initialization. Defaults to 42.\n",
        "        beta (float, optional): Beta value. Defaults to 0.1.\n",
        "        max_length (int, optional): Maximum length. Defaults to 256.\n",
        "        max_prompt_length (int, optional): Maximum prompt length. Defaults to 128.\n",
        "        output_dir (str, optional): Output directory for saving models and logs. Defaults to \"fine-tuned-model\".\n",
        "        push_to_hub (bool, optional): Whether to push to the Hub. Defaults to True.\n",
        "        gradient_accumulation_steps (int, optional): Number of updates steps to accumulate before performing a backward/update pass. Defaults to 4.\n",
        "        warmup_ratio (float, optional): Warmup ratio. Defaults to 0.1.\n",
        "        weight_decay (float, optional): Weight decay. Defaults to 0.0.\n",
        "        lr_scheduler_type (str, optional): Learning rate scheduler type. Defaults to \"linear\".\n",
        "\n",
        "    Returns:\n",
        "        DPOTrainer: The DPO trainer instance.\n",
        "    \"\"\"\n",
        "    gr.Info(\"Getting DPO trainer.\")\n",
        "    try:\n",
        "        # Patch the DPO trainer first\n",
        "        PatchDPOTrainer()\n",
        "\n",
        "        dpo_trainer = DPOTrainer(\n",
        "            model=peft_model,\n",
        "            ref_model=None,\n",
        "            tokenizer=tokenizer,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset,\n",
        "            args=TrainingArguments(\n",
        "                per_device_train_batch_size=per_device_train_batch_size,\n",
        "                num_train_epochs=num_train_epochs,\n",
        "                learning_rate=learning_rate,\n",
        "                logging_steps=logging_steps,\n",
        "                optim=optim,\n",
        "                seed=dpo_trainer_seed,\n",
        "                output_dir=output_dir,\n",
        "                push_to_hub=push_to_hub,\n",
        "                gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "                warmup_ratio=warmup_ratio,\n",
        "                weight_decay=weight_decay,\n",
        "                lr_scheduler_type=lr_scheduler_type,\n",
        "                fp16=not torch.cuda.is_bf16_supported(),\n",
        "                bf16=torch.cuda.is_bf16_supported(),\n",
        "                remove_unused_columns=False,\n",
        "            ),\n",
        "            beta=beta,\n",
        "            max_length=max_length,\n",
        "            max_prompt_length=max_prompt_length,\n",
        "        )\n",
        "        OUTPUT_DIR.value = output_dir\n",
        "\n",
        "        gr.Info(\"DPO trainer is ready.\")\n",
        "\n",
        "        return dpo_trainer\n",
        "\n",
        "    except Exception as e:\n",
        "        raise gr.Error(f\"Error getting DPO trainer: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xC_HzljQx6CP",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Fine-tune model\n",
        "\n",
        "\n",
        "def finetune_model(\n",
        "    dpo_trainer: DPOTrainer,\n",
        ") -> Tuple[LlamaForCausalLM | MistralForCausalLM | GemmaForCausalLM, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Fine-tune a DPO model.\n",
        "\n",
        "    Args:\n",
        "        dpo_trainer (DPOTrainer): The DPO trainer instance.\n",
        "\n",
        "    Returns:\n",
        "        LlamaForCausalLM | MistralForCausalLM | GemmaForCausalLM: The fine-tuned model.\n",
        "        Dict[str, float]: A dictionary containing training results.\n",
        "    \"\"\"\n",
        "    gr.Info(\"Training DPO model.\")\n",
        "    try:\n",
        "        # Fine-tune model\n",
        "        results = dpo_trainer.train()\n",
        "        finetuned_model = dpo_trainer.model\n",
        "        gr.Info(\"Fine-tuning done.\")\n",
        "\n",
        "        return finetuned_model, results\n",
        "\n",
        "    except Exception as e:\n",
        "        raise gr.Error(f\"Error in fine-tuning process: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2X9DqnOn_mA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Save model\n",
        "\n",
        "\n",
        "def save_model(\n",
        "    finetuned_model: LlamaForCausalLM | MistralForCausalLM | GemmaForCausalLM,\n",
        "    token: str,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Save the fine-tuned model to the Hugging Face Hub.\n",
        "\n",
        "    Args:\n",
        "        finetuned_model (LlamaForCausalLM | MistralForCausalLM | GemmaForCausalLM): The fine-tuned model.\n",
        "        token (str): The authentication token for saving the model to the Hugging Face Hub.\n",
        "\n",
        "    Returns:\n",
        "        None.\n",
        "    \"\"\"\n",
        "    gr.Info(\"Saving DPO model.\")\n",
        "    try:\n",
        "        # Save fine-tuned model\n",
        "        finetuned_model.push_to_hub(OUTPUT_DIR.value, token=token)\n",
        "        gr.Info(\"Fine-tuned model successfully saved to Hugging Face.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        raise gr.Error(f\"Error in saving process: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25px9YrsY0yS",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Get training metrics\n",
        "\n",
        "\n",
        "def get_training_metrics(results: DPOTrainer) -> Dataframe:\n",
        "    \"\"\"\n",
        "    Retrieve training metrics from a DPO trainer and format them.\n",
        "\n",
        "    Args:\n",
        "        results (DPOTrainer): The fine-tuned DPO instance.\n",
        "\n",
        "    Returns:\n",
        "        Dataframe: A dataframe containing the formatted training metrics.\n",
        "    \"\"\"\n",
        "    gr.Info(\"Getting and formatting metrics.\")\n",
        "    try:\n",
        "        training_metrics = {\n",
        "            metric: [f\"{value:.2f}\"] for metric, value in results.metrics.items()\n",
        "        }\n",
        "        training_metrics = (\n",
        "            pd.DataFrame(training_metrics, index=[\"Value\"])\n",
        "            .T.reset_index()\n",
        "            .rename(columns={\"index\": \"Metric\"})\n",
        "        )\n",
        "        training_metrics = gr.Dataframe(\n",
        "            value=training_metrics,\n",
        "            interactive=False,\n",
        "            visible=True,\n",
        "        )\n",
        "\n",
        "        gr.Info(\"Metrics successfully retrieved.\")\n",
        "\n",
        "        return training_metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        raise gr.Error(f\"Error in retrieving metrics: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DyNYzGScaPD",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Run all\n",
        "\n",
        "\n",
        "# Run all\n",
        "def run_all(\n",
        "    # get_dataset args\n",
        "    dataset_name,\n",
        "    max_samples,\n",
        "    test_size,\n",
        "    train_test_split_seed,\n",
        "    # get_model args\n",
        "    model_name,\n",
        "    token,\n",
        "    max_seq_length,\n",
        "    load_in_4bit,\n",
        "    # get_peft_mmodel args\n",
        "    r,\n",
        "    lora_alpha,\n",
        "    peft_model_random_state,\n",
        "    target_modules,\n",
        "    loftq_config,\n",
        "    use_rslora,\n",
        "    lora_dropout,\n",
        "    bias,\n",
        "    use_gradient_checkpointing,\n",
        "    # get_dpo_trainer args\n",
        "    per_device_train_batch_size,\n",
        "    num_train_epochs,\n",
        "    learning_rate,\n",
        "    logging_steps,\n",
        "    optim,\n",
        "    dpo_trainer_seed,\n",
        "    beta,\n",
        "    max_length,\n",
        "    max_prompt_length,\n",
        "    output_dir,\n",
        "    push_to_hub,\n",
        "    gradient_accumulation_steps,\n",
        "    warmup_ratio,\n",
        "    weight_decay,\n",
        "    lr_scheduler_type,\n",
        "):\n",
        "    \"\"\"\n",
        "    Run all steps including dataset retrieval, model loading, PEFT model creation, DPO trainer instantiation, model fine-tuning and saving, and training metrics retrieval.\n",
        "\n",
        "    Args:\n",
        "      `get_dataset` args.\n",
        "      `get_model` args.\n",
        "      `get_peft_mmodel` args.\n",
        "      `get_dpo_trainer` args.\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing a sample prompt, its chosen and rejected responses, and the new model's metrics.\n",
        "    \"\"\"\n",
        "    # Get dataset\n",
        "    sample_prompt, sample_chosen, sample_rejected, train_dataset, eval_dataset = (\n",
        "        get_dataset(dataset_name, max_samples, test_size, train_test_split_seed)\n",
        "    )\n",
        "\n",
        "    # Get model\n",
        "    model, tokenizer = get_model(\n",
        "        model_name,\n",
        "        token,\n",
        "        max_seq_length,\n",
        "        load_in_4bit,\n",
        "    )\n",
        "\n",
        "    # Get PEFT model\n",
        "    peft_model = get_peft_model(\n",
        "        model,\n",
        "        r,\n",
        "        lora_alpha,\n",
        "        peft_model_random_state,\n",
        "        target_modules,\n",
        "        loftq_config,\n",
        "        use_rslora,\n",
        "        lora_dropout,\n",
        "        bias,\n",
        "        use_gradient_checkpointing,\n",
        "    )\n",
        "\n",
        "    # Get DPO trainer\n",
        "    dpo_trainer = get_dpo_trainer(\n",
        "        peft_model,\n",
        "        tokenizer,\n",
        "        train_dataset,\n",
        "        eval_dataset,\n",
        "        per_device_train_batch_size,\n",
        "        num_train_epochs,\n",
        "        learning_rate,\n",
        "        logging_steps,\n",
        "        optim,\n",
        "        dpo_trainer_seed,\n",
        "        beta,\n",
        "        max_length,\n",
        "        max_prompt_length,\n",
        "        output_dir,\n",
        "        push_to_hub,\n",
        "        gradient_accumulation_steps,\n",
        "        warmup_ratio,\n",
        "        weight_decay,\n",
        "        lr_scheduler_type,\n",
        "    )\n",
        "\n",
        "    # Fine-tune model\n",
        "    finetuned_model, results = finetune_model(dpo_trainer)\n",
        "\n",
        "    # Save model\n",
        "    save_model(finetuned_model, token)\n",
        "\n",
        "    # Get training metrics\n",
        "    training_metrics = get_training_metrics(results)\n",
        "\n",
        "    # Free memory\n",
        "    del model, tokenizer, peft_model, dpo_trainer, finetuned_model\n",
        "    gc.collect()\n",
        "    gc.collect()\n",
        "\n",
        "    return sample_prompt, sample_chosen, sample_rejected, training_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zm2NCb5LB_1"
      },
      "source": [
        "# **UI**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym34UFIzQwnb",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title UI\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    OUTPUT_DIR = gr.State(\"\")\n",
        "\n",
        "    with gr.Tab(\"DPO fine-tuning\"):\n",
        "        # GET THE DATA ---------------------------------------------------------\n",
        "        gr.Markdown(\"# Get the data\")\n",
        "        with gr.Row():\n",
        "            dataset_name = gr.Dropdown(\n",
        "                choices=dataset_name_list,\n",
        "                value=\"jondurbin/truthy-dpo-v0.1\",\n",
        "                label=\"Dataset name\",\n",
        "                interactive=True,\n",
        "            )\n",
        "            max_samples = gr.Number(\n",
        "                value=1000,\n",
        "                label=\"Maximum samples to retrieve\",\n",
        "                interactive=True,\n",
        "                precision=0,\n",
        "                minimum=100,\n",
        "                step=1000,\n",
        "            )\n",
        "            test_size = gr.Slider(\n",
        "                minimum=0.1,\n",
        "                maximum=0.9,\n",
        "                value=0.2,\n",
        "                step=0.1,\n",
        "                label=\"Test size\",\n",
        "                interactive=True,\n",
        "            )\n",
        "            train_test_split_seed = gr.Number(\n",
        "                value=42,\n",
        "                label=\"Train test split seed\",\n",
        "                interactive=True,\n",
        "                precision=0,\n",
        "                minimum=0,\n",
        "            )\n",
        "\n",
        "        with gr.Accordion(\n",
        "            label=\"Data sample (open to see more when fine-tuning is done)\", open=False\n",
        "        ):\n",
        "            gr.Markdown(\"### Prompt\")\n",
        "            sample_prompt = gr.Textbox(\n",
        "                lines=10,\n",
        "                show_label=False,\n",
        "                container=False,\n",
        "                visible=False,\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"### Chosen\")\n",
        "            sample_chosen = gr.Textbox(\n",
        "                lines=10,\n",
        "                show_label=False,\n",
        "                container=False,\n",
        "                visible=False,\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"### Rejected\")\n",
        "            sample_rejected = gr.Textbox(\n",
        "                lines=10,\n",
        "                show_label=False,\n",
        "                container=False,\n",
        "                visible=False,\n",
        "            )\n",
        "\n",
        "        gr.HTML(\"<br>\")\n",
        "\n",
        "        # GET THE MODEL --------------------------------------------------------\n",
        "        gr.Markdown(\"# Get the model\")\n",
        "        with gr.Row():\n",
        "            model_name = gr.Dropdown(\n",
        "                choices=model_name_list,\n",
        "                value=\"unsloth/zephyr-sft-bnb-4bit\",\n",
        "                label=\"Model name\",\n",
        "                interactive=True,\n",
        "            )\n",
        "            max_seq_length = gr.Number(\n",
        "                value=256,\n",
        "                label=\"Max sequence length (input)\",\n",
        "                interactive=False,\n",
        "            )\n",
        "            token = gr.Textbox(\n",
        "                value=get_token(),\n",
        "                label=\"Hugging Face token\",\n",
        "                interactive=False,\n",
        "                type=\"password\",\n",
        "            )\n",
        "            load_in_4bit = gr.Checkbox(\n",
        "                value=True,\n",
        "                label=\"Load in 4bit\",\n",
        "                interactive=False,\n",
        "            )\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "\n",
        "        # GET THE PEFT MODEL ---------------------------------------------------\n",
        "        with gr.Accordion(\n",
        "            label=\"PEFT config (open to view and edit LoRA adapters)\",\n",
        "            open=False,\n",
        "        ):\n",
        "            with gr.Row():\n",
        "                r = gr.Dropdown(\n",
        "                    choices=exp2_list(3, 6),\n",
        "                    value=2**3,\n",
        "                    label=\"Lora rank\",\n",
        "                    scale=1,\n",
        "                    interactive=True,\n",
        "                )\n",
        "                lora_alpha = gr.Dropdown(\n",
        "                    choices=exp2_list(3, 6),\n",
        "                    value=2**3,\n",
        "                    label=\"Lora alpha\",\n",
        "                    scale=1,\n",
        "                    interactive=True,\n",
        "                )\n",
        "                peft_model_random_state = gr.Number(\n",
        "                    value=42,\n",
        "                    label=\"PEFT model seed\",\n",
        "                    scale=1,\n",
        "                    interactive=True,\n",
        "                    precision=0,\n",
        "                    minimum=0,\n",
        "                )\n",
        "                target_modules = gr.Dropdown(\n",
        "                    choices=target_module_list,\n",
        "                    value=target_module_list,\n",
        "                    multiselect=True,\n",
        "                    label=\"Target modules to be trained (choose one or many)\",\n",
        "                    scale=5,\n",
        "                    interactive=True,\n",
        "                )\n",
        "\n",
        "            with gr.Row():\n",
        "                loftq_config = gr.Textbox(\n",
        "                    value=None,\n",
        "                    label=\"LoRA-Fine-Tuning-Aware Quantization\",\n",
        "                    interactive=False,\n",
        "                    visible=False,\n",
        "                )\n",
        "                use_rslora = gr.Checkbox(\n",
        "                    value=True,\n",
        "                    label=\"Rank-stabilized LoRA\",\n",
        "                    interactive=False,\n",
        "                    visible=False,\n",
        "                )\n",
        "                lora_dropout = gr.Number(\n",
        "                    value=0,\n",
        "                    label=\"LoRA dropout\",\n",
        "                    interactive=False,\n",
        "                    visible=False,\n",
        "                )\n",
        "                bias = gr.Textbox(\n",
        "                    value=\"none\",\n",
        "                    label=\"Bias\",\n",
        "                    interactive=False,\n",
        "                    visible=False,\n",
        "                )\n",
        "                use_gradient_checkpointing = gr.Dropdown(\n",
        "                    choices=use_gradient_checkpointing_list,\n",
        "                    value=\"unsloth\",\n",
        "                    label=\"Use gradient checkpoint\",\n",
        "                    interactive=False,\n",
        "                    visible=False,\n",
        "                )\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "\n",
        "        # GET THE DPO TRAINER --------------------------------------------------\n",
        "        with gr.Accordion(\n",
        "            label=\"DPO trainer (open to view and edit DPO params)\",\n",
        "            open=False,\n",
        "        ):\n",
        "            with gr.Row():\n",
        "                per_device_train_batch_size = gr.Dropdown(\n",
        "                    choices=exp2_list(0, 2),\n",
        "                    value=2**2,\n",
        "                    label=\"Per device train batch size\",\n",
        "                    interactive=True,\n",
        "                )\n",
        "                num_train_epochs = gr.Number(\n",
        "                    value=3,\n",
        "                    label=\"Number of training epochs\",\n",
        "                    interactive=True,\n",
        "                    precision=0,\n",
        "                    minimum=1,\n",
        "                )\n",
        "                learning_rate = gr.Number(\n",
        "                    value=5e-6,\n",
        "                    label=\"Learning rate\",\n",
        "                    interactive=True,\n",
        "                    minimum=0,\n",
        "                )\n",
        "                logging_steps = gr.Number(\n",
        "                    value=50,\n",
        "                    label=\"Logging steps\",\n",
        "                    interactive=True,\n",
        "                    minimum=0,\n",
        "                )\n",
        "                optim = gr.Dropdown(\n",
        "                    choices=optim_list,\n",
        "                    value=optim_list[0],\n",
        "                    label=\"Optimizer\",\n",
        "                    interactive=True,\n",
        "                )\n",
        "            with gr.Row():\n",
        "                dpo_trainer_seed = gr.Number(\n",
        "                    value=42,\n",
        "                    label=\"DPO trainer seed\",\n",
        "                    scale=1,\n",
        "                    interactive=True,\n",
        "                    precision=0,\n",
        "                    minimum=0,\n",
        "                )\n",
        "                beta = gr.Number(\n",
        "                    value=0.1,\n",
        "                    label=\"Beta\",\n",
        "                    scale=1,\n",
        "                    interactive=False,\n",
        "                )\n",
        "                max_length = gr.Number(\n",
        "                    value=256,\n",
        "                    label=\"Max length\",\n",
        "                    scale=1,\n",
        "                    interactive=False,\n",
        "                )\n",
        "                max_prompt_length = gr.Number(\n",
        "                    value=128,\n",
        "                    label=\"Max prompt length\",\n",
        "                    scale=1,\n",
        "                    interactive=False,\n",
        "                )\n",
        "                output_dir = gr.Textbox(\n",
        "                    label=\"HF saving repo\",\n",
        "                    value=\"fine-tuned-model\",\n",
        "                    scale=2,\n",
        "                    interactive=False,\n",
        "                )\n",
        "                with gr.Column(scale=2):\n",
        "                    push_to_hub = gr.Checkbox(\n",
        "                        value=True,\n",
        "                        label=\"Push final model to hub\",\n",
        "                        interactive=False,\n",
        "                    )\n",
        "                    with gr.Row():\n",
        "                        fp16 = gr.Checkbox(\n",
        "                            value=not torch.cuda.is_bf16_supported(),\n",
        "                            label=\"fp16\",\n",
        "                            interactive=False,\n",
        "                        )\n",
        "                        bf16 = gr.Checkbox(\n",
        "                            value=torch.cuda.is_bf16_supported(),\n",
        "                            label=\"bf16\",\n",
        "                            interactive=False,\n",
        "                        )\n",
        "\n",
        "            with gr.Row():\n",
        "                gradient_accumulation_steps = gr.Number(\n",
        "                    value=4,\n",
        "                    label=\"Gradient accumulation steps\",\n",
        "                    interactive=False,\n",
        "                    visible=False,\n",
        "                )\n",
        "                warmup_ratio = gr.Number(\n",
        "                    value=0.1,\n",
        "                    label=\"Warmup ratio\",\n",
        "                    interactive=False,\n",
        "                    visible=False,\n",
        "                )\n",
        "                weight_decay = gr.Number(\n",
        "                    value=0.0,\n",
        "                    label=\"Weight decay\",\n",
        "                    interactive=False,\n",
        "                    visible=False,\n",
        "                )\n",
        "                lr_scheduler_type = gr.Textbox(\n",
        "                    value=\"linear\",\n",
        "                    label=\"LR scheduler type\",\n",
        "                    interactive=False,\n",
        "                    visible=False,\n",
        "                )\n",
        "\n",
        "        gr.HTML(\"<br>\")\n",
        "\n",
        "        # GET THE TRAINING METRICS ---------------------------------------------\n",
        "        training_metrics = gr.Dataframe(\n",
        "            value=pd.DataFrame(),\n",
        "            visible=False,\n",
        "            render=False,\n",
        "        )\n",
        "\n",
        "        # RUN ALL --------------------------------------------------------------\n",
        "        inputs = [\n",
        "            # get_dataset args\n",
        "            dataset_name,\n",
        "            max_samples,\n",
        "            test_size,\n",
        "            train_test_split_seed,\n",
        "            # get_model args\n",
        "            model_name,\n",
        "            token,\n",
        "            max_seq_length,\n",
        "            load_in_4bit,\n",
        "            # get_peft_model args\n",
        "            r,\n",
        "            lora_alpha,\n",
        "            peft_model_random_state,\n",
        "            target_modules,\n",
        "            loftq_config,\n",
        "            use_rslora,\n",
        "            lora_dropout,\n",
        "            bias,\n",
        "            use_gradient_checkpointing,\n",
        "            # get_dpo_trainer args\n",
        "            per_device_train_batch_size,\n",
        "            num_train_epochs,\n",
        "            learning_rate,\n",
        "            logging_steps,\n",
        "            optim,\n",
        "            dpo_trainer_seed,\n",
        "            beta,\n",
        "            max_length,\n",
        "            max_prompt_length,\n",
        "            output_dir,\n",
        "            push_to_hub,\n",
        "            gradient_accumulation_steps,\n",
        "            warmup_ratio,\n",
        "            weight_decay,\n",
        "            lr_scheduler_type,\n",
        "        ]\n",
        "        outputs = [\n",
        "            sample_prompt,\n",
        "            sample_chosen,\n",
        "            sample_rejected,\n",
        "            training_metrics,\n",
        "        ]\n",
        "\n",
        "        gr.Markdown(\"# Fine-tune your model on your data\")\n",
        "        run_all_button = gr.Button(value=\"Run DPO fine-tuning\")\n",
        "        run_all_button.click(fn=run_all, inputs=inputs, outputs=outputs)\n",
        "        gc.collect()\n",
        "\n",
        "    with gr.Tab(\"Training metrics\"):\n",
        "        training_metrics.render()\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}